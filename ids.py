#!/usr/bin/env python3# -*- coding: utf-8 -*-import osimport pandas as pdimport numpy as npfrom datetime import datetimefrom scapy.all import sniff, IP, TCP, UDP, ICMP, Rawfrom sklearn.preprocessing import LabelEncoder, StandardScalerimport tensorflow as tfimport pyfiglet  # for console ASCII logo# ----------------- CONFIG -----------------MODEL_PATH = "ids_v0.h5"TRAIN_DATA_PATH = "DB/Train_data.csv"ALL_TRAFFIC_LOG = "traffic_all.log"ANOMALY_LOG = "traffic_anomalies.log"selected_features = [    "serror_rate", "same_srv_rate", "logged_in", "dst_host_srv_count",    "flag", "protocol_type", "service", "count", "dst_host_same_srv_rate",    "diff_srv_rate", "src_bytes", "dst_bytes"]categorical_cols = ["flag", "protocol_type", "service"]# -------------------------------------------# Load training data for preprocessing fittrain_df = pd.read_csv(TRAIN_DATA_PATH)# Fit LabelEncoders from training datalabel_encoders = {}for col in categorical_cols:    le = LabelEncoder()    le.fit(train_df[col])    label_encoders[col] = le# Fit StandardScaler from training datascaler = StandardScaler()train_encoded = train_df.copy()for col in categorical_cols:    train_encoded[col] = label_encoders[col].transform(train_encoded[col])scaler.fit(train_encoded[selected_features])# Load trained modelmodel = tf.keras.models.load_model(MODEL_PATH)# ---------------- Utility Functions ----------------# Port â†’ service mappingdef get_service(port):    common_services = {        80: "http", 443: "https", 21: "ftp", 22: "ssh", 25: "smtp",        53: "domain", 110: "pop3", 143: "imap"    }    return common_services.get(port, "other")def get_protocol(pkt):    if TCP in pkt:        return "tcp"    elif UDP in pkt:        return "udp"    elif ICMP in pkt:        return "icmp"    else:        return "other"# Extract features (placeholder rates)def extract_features(pkt):    try:        proto = get_protocol(pkt)        service = "other"        src_bytes = len(pkt[Raw].load) if Raw in pkt else 0        dst_bytes = 0        serror_rate = 0        same_srv_rate = 0        logged_in = 0        dst_host_srv_count = 0        dst_host_same_srv_rate = 0        diff_srv_rate = 0        count = 0        flag = "SF" if TCP in pkt and pkt[TCP].flags == "S" else "OTH"        sport = pkt.sport if TCP in pkt or UDP in pkt else 0        dport = pkt.dport if TCP in pkt or UDP in pkt else 0        if dport != 0:            service = get_service(dport)        features = {            "serror_rate": serror_rate,            "same_srv_rate": same_srv_rate,            "logged_in": logged_in,            "dst_host_srv_count": dst_host_srv_count,            "flag": flag,            "protocol_type": proto,            "service": service,            "count": count,            "dst_host_same_srv_rate": dst_host_same_srv_rate,            "diff_srv_rate": diff_srv_rate,            "src_bytes": src_bytes,            "dst_bytes": dst_bytes        }        meta = {            "ts": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),            "src_ip": pkt[IP].src if IP in pkt else None,            "dst_ip": pkt[IP].dst if IP in pkt else None,            "sport": sport,            "dport": dport,            "protocol": proto,            "pkt_len": len(pkt)        }        return features, meta    except Exception as e:        print(f"[!] Feature extraction error: {e}")        return None, None# Logging functionsdef log_all(meta, score, label):    with open(ALL_TRAFFIC_LOG, "a") as f:        f.write(f"{meta['ts']} | {meta['src_ip']}:{meta['sport']} -> {meta['dst_ip']}:{meta['dport']} "                f"| proto={meta['protocol']} | len={meta['pkt_len']} | score={score:.4f} | label={label}\n")def log_anomaly(meta, features, score):    with open(ANOMALY_LOG, "a") as f:        f.write(f"{meta['ts']} | score={score:.4f} | src={meta['src_ip']}:{meta['sport']} -> "                f"dst={meta['dst_ip']}:{meta['dport']} | proto={meta['protocol']} | features={features}\n")    print(f"ðŸš¨ Anomaly detected! Score={score:.4f}, Meta={meta}")# Process each packetdef process_packet(pkt):    feat_dict, meta = extract_features(pkt)    if feat_dict is None:        return    df = pd.DataFrame([feat_dict])    for col in categorical_cols:        if df[col].iloc[0] not in label_encoders[col].classes_:            df[col] = "__unknown__"            if "__unknown__" not in label_encoders[col].classes_:                label_encoders[col].classes_ = np.append(label_encoders[col].classes_, "__unknown__")        df[col] = label_encoders[col].transform(df[col])    X_scaled = scaler.transform(df[selected_features])    pred = model.predict(X_scaled, verbose=0)[0][0]  # verbose=0 stops TF output    label = "anomaly" if pred > 0.5 else "normal"    # Print prediction only    print(f"[PREDICTION] Score={pred:.6f} | Src={meta['src_ip']} -> Dst={meta['dst_ip']} | Proto={meta['protocol']}")    log_all(meta, pred, label)    if label == "anomaly":        log_anomaly(meta, feat_dict, pred)# ----------------- MAIN -----------------# Print ASCII logo using pyfigletascii_logo = pyfiglet.figlet_format("AI-IDS")print(ascii_logo)print("ðŸš€ Live Network Anomaly Detection Started...\n")# Start sniffingprint("[*] Starting live capture... Press Ctrl+C to stop.")try:    sniff(prn=process_packet, store=False)except KeyboardInterrupt:    print("\n[!] Capture stopped.")